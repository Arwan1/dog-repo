{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/radhakrishnan-omotec/arwan-iris-dog-repo/blob/main/ISEF_FINAL_ArwanMakhija_Prediction_EfficientNet_V2%E2%80%91L_CNN_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "qMquMI4EPOei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enhanced Python Notebook for **TailSense** : EfficientNet V2â€‘L based Canine Pet  Audio Spectrogram Classification\n",
        "\n",
        "### Author : ARWAAN MAKHIJA"
      ],
      "metadata": {
        "id": "2eavEqsxZITr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is an enhanced Python notebook implementation for Google Colab that integrates both image classification and spectrogram audio classification using the EfficientNet V2â€‘L model, optimized for maximum accuracy and depth.\n",
        "\n",
        "It leverages EfficientNet V2â€‘Lâ€™s deep architecture (~60M parameters) with residual connections for classifying dog emotions from both image and audio data derived from videos of a Cocker Spaniel.\n",
        "\n",
        "The dataset is assumed to contain 8-10 emotion classes (e.g., \"defensive,\" \"stressed,\" \"friendly\"), and the implementation includes data preprocessing, model training, evaluation, a Gradio interface for real-time inference, and TensorFlow Lite conversion for edge deployment."
      ],
      "metadata": {
        "id": "OKtmg6QfTHE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EfficientNet V2â€‘L-based Spectrogram Audio Classification"
      ],
      "metadata": {
        "id": "hm0a5UB_TJ7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2** : Cocker Spaniel 8 Emotions Prediction EfficientNet V2â€‘L-based Spectrogram Audio Classification\n"
      ],
      "metadata": {
        "id": "E-1xViExRpra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8-class Cocker Spaniel emotion classification using spectrogram images with EfficientNetV2-L for maximum accuracy."
      ],
      "metadata": {
        "id": "KNrKfGTtZmBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "m8lvEkuETUTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# My Project Key Enhancements:\n",
        "**EfficientNetV2-L for Maximum Accuracy:**<br>\n",
        "Replaced EfficientNetB0 with EfficientNetV2-L (~118M parameters), the largest EfficientNet model in TensorFlowâ€™s V2 family, designed for high accuracy with deeper layers and advanced scaling (width, depth, resolution).<br>\n",
        "Used ImageNet weights for transfer learning, freezing the base model initially to leverage pre-trained features.<br>\n",
        "Fine-tuned the last 30 layers (as in the original EfficientNet V2-L code) to adapt to spectrogram-specific patterns, balancing computational cost and accuracy.<br>\n",
        "Adjusted input size to 480x480, the default for EfficientNetV2-L, to maximize feature extraction (increased from 224x224).<br>\n",
        "Used tf.keras.applications.efficientnet_v2.preprocess_input for model-specific preprocessing, ensuring spectrogram images are normalized correctly.<br><br>\n",
        "**Custom Head for Deep Features:**<br>\n",
        "Increased dense layer sizes to 2048 and 1024 units (from 512 and 256) to handle the richer feature representations from EfficientNetV2-Lâ€™s deeper architecture.\n",
        "Retained dropout rates (0.5 and 0.3) to prevent overfitting, given the modelâ€™s high parameter count.<br><br>\n",
        "**Optimized Training:**<br>\n",
        "Set initial learning rate to 1e-4 and fine-tuning learning rate to 1e-5 for stable convergence with the larger model.<br>\n",
        "Extended fine-tuning to 15 epochs (from 10) to fully leverage EfficientNetV2-Lâ€™s capacity, while keeping EarlyStopping to prevent overfitting.\n",
        "Reduced batch size to 16 (from 32) to accommodate the larger input size (480x480) and model complexity within Colabâ€™s GPU memory constraints.<br><br>\n",
        "**Retained Enhancements:**<br>\n",
        "Dataset: Kept the 8-class Cocker Spaniel emotion dataset (Sad, Happy, Stress, Restless, Normal, Love, Unhappy, Tired) and spectrogram pipeline (Librosa for mel-spectrograms).<br>\n",
        "Data Augmentation: Preserved advanced augmentation (rotation_range=30, vertical_flip=True, brightness_range=[0.8, 1.2]) to enhance generalization for spectrogram images.<br>\n",
        "Learning Rate Scheduling: Retained ReduceLROnPlateau (factor=0.5, patience=5, min_lr=1e-6) for adaptive optimization.<br>\n",
        "Grad-CAM: Kept Grad-CAM for interpretability, using the top_conv layer (last convolutional layer in EfficientNetV2-L), allowing visualization of spectrogram regions critical for emotion predictions.<br>\n",
        "Gradio Interface: Maintained the audio-to-spectrogram prediction pipeline, accepting .wav inputs and displaying emotion predictions with confidence scores.\n",
        "TensorFlow Lite: Preserved float16 quantization for edge deployment on devices like Raspberry Pi 5.<br>\n",
        "Modular Code: Retained functions (split_dataset, plot_training_metrics, plot_confusion_matrix, display_gradcam) for clarity and reusability.<br><br>\n",
        "**Alignment with Previous Code:**<br>\n",
        "Built on the original EfficientNet V2-L implementation from rpi_testing_isef1_efficientnet_dog_emotion_classification.py, adopting its fine-tuning strategy (last 30 layers) and dense layer structure (2048, 1024 units).<br>\n",
        "Integrated the templateâ€™s (Cocker_Spaniel_Emotion_ResNet152_Training.ipynb) dataset structure, data splitting, and evaluation metrics (confusion matrix, classification report).<br>\n",
        "Ensured compatibility with Google Colabâ€™s GPU environment, optimizing for the larger modelâ€™s computational demands.<br><br>"
      ],
      "metadata": {
        "id": "LGrjqwWBOjIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notes:**<br>\n",
        "The code assumes an audio dataset at /content/drive/MyDrive/Cocker_Spaniel_Emotions_Audio with subfolders for each emotion (Sad, Happy, etc.) containing .wav files. Users must provide this dataset and update paths accordingly.<br>\n",
        "The Grad-CAM implementation uses the top_conv layer, the last convolutional layer in EfficientNetV2-L. Users should verify this using model.summary() if the architecture differs.<br>\n",
        "The input size is set to 480x480, optimal for EfficientNetV2-L, but may require significant GPU memory. Users with limited resources can reduce to 384x384, though this may slightly impact accuracy.<br>\n",
        "Training epochs are set to 50 (initial) + 15 (fine-tuning), with EarlyStopping to prevent overfitting. Users may adjust based on dataset size and convergence.\n",
        "The batch size is reduced to 16 to fit within Colabâ€™s GPU memory for the larger model and input size. Users with high-end GPUs can increase it for faster training.<br>\n",
        "The TFLite model is optimized with float16 quantization, but EfficientNetV2-Lâ€™s size may challenge edge devices like Raspberry Pi 5. Users may need to test performance or consider model pruning for deployment.<br>"
      ],
      "metadata": {
        "id": "P_CNRp5qZ430"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "UPe9-av2PNVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cocker_Spaniel_Emotion_EfficientNetV2-L\n",
        "\n",
        "### Enhanced for 8-class Cocker Spaniel emotion classification using spectrogram images with EfficientNetV2-L for maximum accuracy."
      ],
      "metadata": {
        "id": "nV4Rl-1KPV0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Setup and Import Libraries\n",
        "# **Step 1**:"
      ],
      "metadata": {
        "id": "JNL_G7bNPctL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Step 1: Setup and Import Libraries\n",
        "# =============================\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.applications import EfficientNetV2L\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import librosa\n",
        "import librosa.display\n",
        "import gradio as gr\n",
        "from google.colab import drive\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import pathlib\n",
        "import seaborn as sns\n",
        "import shutil\n",
        "import random"
      ],
      "metadata": {
        "id": "gCCj-w8JPMFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Enable GPU acceleration"
      ],
      "metadata": {
        "id": "PzD1s-C2Pg_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable GPU acceleration\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU available:\", tf.test.is_gpu_available())"
      ],
      "metadata": {
        "id": "tRYM--mmP9ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Mount Google Drive"
      ],
      "metadata": {
        "id": "w7wa8xjnPgUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mReB9z9YP5tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Define Dataset Paths and Convert Audio to Spectrogram Images\n",
        "# **Step 2**:"
      ],
      "metadata": {
        "id": "sSLwJfC4Phf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Step 2: Define Dataset Paths and Convert Audio to Spectrogram Images\n",
        "# =============================\n",
        "audio_dataset_dir = '/content/drive/MyDrive/Cocker_Spaniel_Emotions_Audio'\n",
        "spectrogram_dataset_dir = '/content/drive/MyDrive/Cocker_Spaniel_Emotions_Spectrogram'\n",
        "\n",
        "os.makedirs(spectrogram_dataset_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "Z_1FVdDiQBqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Define emotion classes"
      ],
      "metadata": {
        "id": "jSyEMlLjPiAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define emotion classes\n",
        "emotion_classes = [\"Sad\", \"Happy\", \"Stress\", \"Restless\", \"Normal\", \"Love\", \"Unhappy\", \"Tired\"]\n",
        "class_labels = {cls: idx for idx, cls in enumerate(emotion_classes)}"
      ],
      "metadata": {
        "id": "kiCEpXr4QFUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Convert audio files to spectrogram images"
      ],
      "metadata": {
        "id": "WVpvJ_b3Pi_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert audio files to spectrogram images\n",
        "def generate_spectrogram(audio_path, output_image_path):\n",
        "    \"\"\"Convert an audio file into a mel-spectrogram image.\"\"\"\n",
        "    y, sr = librosa.load(audio_path, sr=44100)\n",
        "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel')\n",
        "    plt.axis('off')\n",
        "    plt.savefig(output_image_path, bbox_inches='tight', pad_inches=0)\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "MOxsR_XOQJHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Process audio dataset"
      ],
      "metadata": {
        "id": "OIR8jH4DPjmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process audio dataset\n",
        "for emotion in emotion_classes:\n",
        "    emotion_path = os.path.join(audio_dataset_dir, emotion)\n",
        "    output_emotion_folder = os.path.join(spectrogram_dataset_dir, emotion)\n",
        "    os.makedirs(output_emotion_folder, exist_ok=True)\n",
        "    if os.path.isdir(emotion_path):\n",
        "        for audio_file in os.listdir(emotion_path):\n",
        "            if audio_file.endswith('.wav'):\n",
        "                audio_path = os.path.join(emotion_path, audio_file)\n",
        "                output_image_path = os.path.join(output_emotion_folder, f\"{os.path.splitext(audio_file)[0]}.png\")\n",
        "                generate_spectrogram(audio_path, output_image_path)\n",
        "print(\"âœ… Spectrogram dataset generated successfully.\")"
      ],
      "metadata": {
        "id": "3AidPRXNQNZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Count total spectrogram image"
      ],
      "metadata": {
        "id": "DMshQiRjPkQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count total spectrogram images\n",
        "dataset_dir = pathlib.Path(spectrogram_dataset_dir)\n",
        "total_images = len(list(dataset_dir.glob('*/*.png')))\n",
        "print(f\"Total spectrogram images in dataset: {total_images}\")"
      ],
      "metadata": {
        "id": "PJ9lKkaMQQ1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Count images per class"
      ],
      "metadata": {
        "id": "3t-EzeLePkzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count images per class\n",
        "image_counts = {}\n",
        "image_paths = {}\n",
        "for cls in emotion_classes:\n",
        "    count = len(list(dataset_dir.glob(f'{cls}/*')))\n",
        "    paths = list(dataset_dir.glob(f'{cls}/*'))\n",
        "    image_counts[cls] = count\n",
        "    image_paths[cls] = paths\n",
        "    print(f\"{cls}: {count} images\")"
      ],
      "metadata": {
        "id": "HVirfIB4QUe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Plot class distribution"
      ],
      "metadata": {
        "id": "rFdu-PxmPlfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot class distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(image_counts.keys(), image_counts.values(), color='skyblue')\n",
        "plt.xlabel('Emotion Class')\n",
        "plt.ylabel('Number of Spectrogram Images')\n",
        "plt.title('Distribution of Spectrogram Images by Emotion')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "siRC-InyQYMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Split Dataset into Train, Validation, and Test Sets\n",
        "# **Step 3**:"
      ],
      "metadata": {
        "id": "Ix0WQXj-PmDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Step 3: Split Dataset into Train, Validation, and Test Sets\n",
        "# =============================\n",
        "def split_dataset(dataset_dir, output_dir, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        os.makedirs(os.path.join(output_dir, split), exist_ok=True)\n",
        "\n",
        "    for cls in emotion_classes:\n",
        "        class_path = os.path.join(dataset_dir, cls)\n",
        "        images = os.listdir(class_path)\n",
        "        random.shuffle(images)\n",
        "\n",
        "        total_images = len(images)\n",
        "        train_end = int(train_ratio * total_images)\n",
        "        val_end = train_end + int(val_ratio * total_images)\n",
        "\n",
        "        train_images = images[:train_end]\n",
        "        val_images = images[train_end:val_end]\n",
        "        test_images = images[val_end:]\n",
        "\n",
        "        def copy_images(image_list, split):\n",
        "            split_class_dir = os.path.join(output_dir, split, cls)\n",
        "            os.makedirs(split_class_dir, exist_ok=True)\n",
        "            for img in image_list:\n",
        "                src = os.path.join(class_path, img)\n",
        "                dst = os.path.join(split_class_dir, img)\n",
        "                shutil.copy(src, dst)\n",
        "\n",
        "        copy_images(train_images, 'train')\n",
        "        copy_images(val_images, 'val')\n",
        "        copy_images(test_images, 'test')\n",
        "\n",
        "splitted_dataset_dir = '/content/drive/MyDrive/Cocker_Spaniel_Emotions_Splitted'\n",
        "split_dataset(dataset_dir, splitted_dataset_dir)\n",
        "print(\"âœ… Dataset successfully split into training, validation, and testing sets!\")"
      ],
      "metadata": {
        "id": "JHjHCwYBQdDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Verify split counts"
      ],
      "metadata": {
        "id": "EMba2W3kPmYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify split counts\n",
        "for split in ['train', 'val', 'test']:\n",
        "    print(f\"\\nðŸ“‚ {split.upper()} SET:\")\n",
        "    split_path = os.path.join(splitted_dataset_dir, split)\n",
        "    for cls in emotion_classes:\n",
        "        class_path = os.path.join(split_path, cls)\n",
        "        num_images = len(os.listdir(class_path)) if os.path.exists(class_path) else 0\n",
        "        print(f\"   - {cls}: {num_images} images\")"
      ],
      "metadata": {
        "id": "Vd_ojHhVQhWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Data Augmentation and Generators\n",
        "# **Step 4**:"
      ],
      "metadata": {
        "id": "EE-AcTLdPm1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Step 4: Data Augmentation and Generators\n",
        "# =============================\n",
        "IMG_HEIGHT, IMG_WIDTH, BATCH_SIZE = 480, 480, 16  # Increased input size for EfficientNetV2-L\n",
        "NUM_CLASSES = 8\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    fill_mode='nearest',\n",
        "    preprocessing_function=tf.keras.applications.efficientnet_v2.preprocess_input\n",
        ")\n",
        "\n",
        "val_test_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    preprocessing_function=tf.keras.applications.efficientnet_v2.preprocess_input\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    f'{splitted_dataset_dir}/train',\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_test_datagen.flow_from_directory(\n",
        "    f'{splitted_dataset_dir}/val',\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = val_test_datagen.flow_from_directory(\n",
        "    f'{splitted_dataset_dir}/test',\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(\"\\nClass indices:\", train_generator.class_indices)"
      ],
      "metadata": {
        "id": "eVeLSNv0Ql8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) Define EfficientNetV2-L Model\n",
        "# **Step 5**:"
      ],
      "metadata": {
        "id": "ezUbLwV9Pncy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Step 5: Define EfficientNetV2-L Model\n",
        "# =============================\n",
        "def create_efficientnetv2l_model(num_classes):\n",
        "    base_model = EfficientNetV2L(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
        "    base_model.trainable = False  # Freeze base model layers\n",
        "\n",
        "    inputs = base_model.input\n",
        "    x = base_model.output\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(2048, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(1024, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "model = create_efficientnetv2l_model(NUM_CLASSES)\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "RKcqGMkKQw8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Define callbacks"
      ],
      "metadata": {
        "id": "XSxBDBvtPoFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define callbacks\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    '/content/drive/MyDrive/Cocker_Spaniel_Emotions/efficientnetv2l_spectrogram_best.h5',\n",
        "    monitor='val_accuracy', save_best_only=True\n",
        ")\n",
        "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)"
      ],
      "metadata": {
        "id": "5VCn72AtQ2MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Train the model"
      ],
      "metadata": {
        "id": "SkouDckLQtkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=50,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stopping, checkpoint, lr_scheduler]\n",
        ")"
      ],
      "metadata": {
        "id": "zmNryqY5Q5cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Fine-tune: Unfreeze last 30 layers"
      ],
      "metadata": {
        "id": "S9KTA0CrQt28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune: Unfreeze last 30 layers\n",
        "base_model = model.layers[0]\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-30]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "fine_tune_epochs = 15\n",
        "history_fine = model.fit(\n",
        "    train_generator,\n",
        "    epochs=fine_tune_epochs,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stopping, checkpoint]\n",
        ")"
      ],
      "metadata": {
        "id": "b7LxohOLRA5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Evaluate on test set"
      ],
      "metadata": {
        "id": "emLT3OztQtvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test set\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(f\"\\nâœ… Test Accuracy: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "mGCNA-aqQ852"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6) Plot Training Metrics\n",
        "# **Step 6**:"
      ],
      "metadata": {
        "id": "2gBqTgvbb1VG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Step 6: Plot Training Metrics\n",
        "# =============================\n",
        "def plot_training_metrics(history, history_fine=None):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    if history_fine:\n",
        "        acc += history_fine.history['accuracy']\n",
        "        val_acc += history_fine.history['val_accuracy']\n",
        "        loss += history_fine.history['loss']\n",
        "        val_loss += history_fine.history['val_loss']\n",
        "\n",
        "    epochs_range = range(len(acc))\n",
        "\n",
        "    plt.figure(figsize=(14, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, acc, label='Training Accuracy', marker='o')\n",
        "    plt.plot(epochs_range, val_acc, label='Validation Accuracy', marker='x')\n",
        "    plt.title('ðŸ“ˆ Training & Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, loss, label='Training Loss', marker='o', linestyle='--')\n",
        "    plt.plot(epochs_range, val_loss, label='Validation Loss', marker='x', linestyle='--')\n",
        "    plt.title('ðŸ“‰ Training & Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_metrics(history, history_fine)"
      ],
      "metadata": {
        "id": "HvyHYLypbza4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7) Confusion Matrix and Classification Report\n",
        "# **Step 7**:"
      ],
      "metadata": {
        "id": "5jtQ-R0MQt8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Step 7: Confusion Matrix and Classification Report\n",
        "# =============================\n",
        "def plot_confusion_matrix(model, test_generator):\n",
        "    class_names = list(test_generator.class_indices.keys())\n",
        "    y_pred_probs = model.predict(test_generator)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "    y_true = test_generator.classes\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Confusion Matrix ðŸ“Š')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "plot_confusion_matrix(model, test_generator)"
      ],
      "metadata": {
        "id": "86MCkbGcRG0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8) Grad-CAM for interpretability\n",
        "# **Step 8**:"
      ],
      "metadata": {
        "id": "FG6Lp2rsQt_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Step 8: Grad-CAM for Interpretability\n",
        "# =============================\n",
        "def get_gradcam_heatmap(model, img_array, last_conv_layer_name):\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
        "    )\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(img_array)\n",
        "        predicted_class = tf.argmax(predictions[0])\n",
        "        class_output = predictions[:, predicted_class]\n",
        "\n",
        "    grads = tape.gradient(class_output, conv_outputs)\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    conv_outputs = conv_outputs[0]\n",
        "    heatmap = tf.reduce_mean(tf.multiply(conv_outputs, pooled_grads), axis=-1)\n",
        "    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n",
        "    return heatmap\n",
        "\n",
        "def display_gradcam(img_path, model, last_conv_layer_name):\n",
        "    img = load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = tf.keras.applications.efficientnet_v2.preprocess_input(img_array)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    heatmap = get_gradcam_heatmap(model, img_array, last_conv_layer_name)\n",
        "    heatmap = cv2.resize(heatmap, (IMG_WIDTH, IMG_HEIGHT))\n",
        "\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "\n",
        "    superimposed_img = heatmap * 0.4 + img\n",
        "    superimposed_img = np.clip(superimposed_img, 0, 255).astype(np.uint8)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    plt.title('Original Spectrogram')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB))\n",
        "    plt.title('Grad-CAM Heatmap')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "pMRPpPw5RLSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Grad-CAM visualization"
      ],
      "metadata": {
        "id": "sL9TrzJyQuCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grad-CAM visualization\n",
        "img_path = '/content/drive/MyDrive/Cocker_Spaniel_Emotions_Splitted/val/Happy/sample_spec.png'  # Update with actual path\n",
        "display_gradcam(img_path, model, 'top_conv')  # Last conv layer in EfficientNetV2-L"
      ],
      "metadata": {
        "id": "MG2un-VQRQB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9) Gradio Interface for Real-Time Prediction\n",
        "# **Step 9**:\n"
      ],
      "metadata": {
        "id": "9OeK8EfjcIjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Step 9: Gradio Interface for Real-Time Prediction\n",
        "# =============================\n",
        "def predict_emotion(input_audio):\n",
        "    # Save uploaded audio to a temporary file\n",
        "    audio_path = \"temp_audio.wav\"\n",
        "    with open(audio_path, \"wb\") as f:\n",
        "        f.write(input_audio.read())\n",
        "\n",
        "    # Convert audio to spectrogram\n",
        "    spec_path = \"temp_spec.png\"\n",
        "    generate_spectrogram(audio_path, spec_path)\n",
        "\n",
        "    # Preprocess spectrogram image\n",
        "    spec_img = load_img(spec_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
        "    spec_array = img_to_array(spec_img)\n",
        "    spec_array = tf.keras.applications.efficientnet_v2.preprocess_input(spec_array)\n",
        "    spec_array = np.expand_dims(spec_array, axis=0)\n",
        "\n",
        "    # Predict emotion\n",
        "    pred = model.predict(spec_array)\n",
        "    emotion = emotion_classes[np.argmax(pred)]\n",
        "    confidence = np.max(pred) * 100\n",
        "\n",
        "    return f\"Predicted Emotion: {emotion} ({confidence:.2f}%)\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Cocker Spaniel Emotion Classification from Audio Spectrograms\")\n",
        "    audio_input = gr.Audio(label=\"Record/Upload Audio\", type=\"file\")\n",
        "    predict_button = gr.Button(\"Predict\")\n",
        "    output = gr.Textbox(label=\"Prediction\")\n",
        "\n",
        "    predict_button.click(predict_emotion, inputs=audio_input, outputs=output)\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "-UbxO0RXcMeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10) Convert to TensorFlow Lite\n",
        "# **Step 10**:"
      ],
      "metadata": {
        "id": "3Uig21LycQ6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Step 10: Convert to TensorFlow Lite\n",
        "# =============================\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "tflite_model = converter.convert()\n",
        "with open('/content/drive/MyDrive/Cocker_Spaniel_Emotions/efficientnetv2l_spectrogram.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "print(\"âœ… TFLite model saved successfully.\")"
      ],
      "metadata": {
        "id": "SDe86nHwcPrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Save the EfficientNetV2-L model"
      ],
      "metadata": {
        "id": "o2QQPwOnQuF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the full EfficientNetV2-L model\n",
        "model.save('/content/drive/MyDrive/Cocker_Spaniel_Emotions/efficientnetv2l_spectrogram_final.h5')\n",
        "print(\"âœ… Model saved successfully as 'efficientnetv2l_spectrogram_final.h5'\")"
      ],
      "metadata": {
        "id": "UY7jIoh4RS5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Test prediction\n",
        "## Test prediction on a single image"
      ],
      "metadata": {
        "id": "_bgd_L-pQuI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test prediction on a single image\n",
        "def predict_single_image(img_path, model):\n",
        "    img = image.load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
        "\n",
        "    prediction = model.predict(img_array)\n",
        "    predicted_class = emotion_classes[np.argmax(prediction)]\n",
        "\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Predicted Emotion: {predicted_class}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "predict_single_image(img_path, model)"
      ],
      "metadata": {
        "id": "C1kzbOlTRbkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "---\n"
      ],
      "metadata": {
        "id": "j_1SC8RuRgcK"
      }
    }
  ]
}