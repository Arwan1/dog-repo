{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/radhakrishnan-omotec/arwan-iris-dog-repo/blob/main/ISEF_FINAL_ArwanMakhija_Prediction_EfficientNet_V2%E2%80%91L_CNN_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "qMquMI4EPOei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enhanced Python Notebook for **TailSense** : EfficientNet V2‑L based Canine Pet  Audio Spectrogram Classification\n",
        "\n",
        "### Author : ARWAAN MAKHIJA"
      ],
      "metadata": {
        "id": "2eavEqsxZITr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is an enhanced Python notebook implementation for Google Colab that integrates both image classification and spectrogram audio classification using the EfficientNet V2‑L model, optimized for maximum accuracy and depth.\n",
        "\n",
        "It leverages EfficientNet V2‑L’s deep architecture (~60M parameters) with residual connections for classifying dog emotions from both image and audio data derived from videos of a Cocker Spaniel.\n",
        "\n",
        "The dataset is assumed to contain 8-10 emotion classes (e.g., \"defensive,\" \"stressed,\" \"friendly\"), and the implementation includes data preprocessing, model training, evaluation, a Gradio interface for real-time inference, and TensorFlow Lite conversion for edge deployment."
      ],
      "metadata": {
        "id": "OKtmg6QfTHE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EfficientNet V2‑L-based Spectrogram Audio Classification"
      ],
      "metadata": {
        "id": "hm0a5UB_TJ7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2** : Cocker Spaniel 8 Emotions Prediction EfficientNet V2‑L-based Spectrogram Audio Classification\n"
      ],
      "metadata": {
        "id": "E-1xViExRpra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8-class Cocker Spaniel emotion classification using spectrogram images with EfficientNetV2-L for maximum accuracy."
      ],
      "metadata": {
        "id": "KNrKfGTtZmBI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "m8lvEkuETUTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# My Project Key Enhancements:\n",
        "**EfficientNetV2-L for Maximum Accuracy:**<br>\n",
        "Replaced EfficientNetB0 with EfficientNetV2-L (~118M parameters), the largest EfficientNet model in TensorFlow’s V2 family, designed for high accuracy with deeper layers and advanced scaling (width, depth, resolution).<br>\n",
        "Used ImageNet weights for transfer learning, freezing the base model initially to leverage pre-trained features.<br>\n",
        "Fine-tuned the last 30 layers (as in the original EfficientNet V2-L code) to adapt to spectrogram-specific patterns, balancing computational cost and accuracy.<br>\n",
        "Adjusted input size to 480x480, the default for EfficientNetV2-L, to maximize feature extraction (increased from 224x224).<br>\n",
        "Used tf.keras.applications.efficientnet_v2.preprocess_input for model-specific preprocessing, ensuring spectrogram images are normalized correctly.<br><br>\n",
        "**Custom Head for Deep Features:**<br>\n",
        "Increased dense layer sizes to 2048 and 1024 units (from 512 and 256) to handle the richer feature representations from EfficientNetV2-L’s deeper architecture.\n",
        "Retained dropout rates (0.5 and 0.3) to prevent overfitting, given the model’s high parameter count.<br><br>\n",
        "**Optimized Training:**<br>\n",
        "Set initial learning rate to 1e-4 and fine-tuning learning rate to 1e-5 for stable convergence with the larger model.<br>\n",
        "Extended fine-tuning to 15 epochs (from 10) to fully leverage EfficientNetV2-L’s capacity, while keeping EarlyStopping to prevent overfitting.\n",
        "Reduced batch size to 16 (from 32) to accommodate the larger input size (480x480) and model complexity within Colab’s GPU memory constraints.<br><br>\n",
        "**Retained Enhancements:**<br>\n",
        "Dataset: Kept the 8-class Cocker Spaniel emotion dataset (Sad, Happy, Stress, Restless, Normal, Love, Unhappy, Tired) and spectrogram pipeline (Librosa for mel-spectrograms).<br>\n",
        "Data Augmentation: Preserved advanced augmentation (rotation_range=30, vertical_flip=True, brightness_range=[0.8, 1.2]) to enhance generalization for spectrogram images.<br>\n",
        "Learning Rate Scheduling: Retained ReduceLROnPlateau (factor=0.5, patience=5, min_lr=1e-6) for adaptive optimization.<br>\n",
        "Grad-CAM: Kept Grad-CAM for interpretability, using the top_conv layer (last convolutional layer in EfficientNetV2-L), allowing visualization of spectrogram regions critical for emotion predictions.<br>\n",
        "Gradio Interface: Maintained the audio-to-spectrogram prediction pipeline, accepting .wav inputs and displaying emotion predictions with confidence scores.\n",
        "TensorFlow Lite: Preserved float16 quantization for edge deployment on devices like Raspberry Pi 5.<br>\n",
        "Modular Code: Retained functions (split_dataset, plot_training_metrics, plot_confusion_matrix, display_gradcam) for clarity and reusability.<br><br>\n",
        "**Alignment with Previous Code:**<br>\n",
        "Built on the original EfficientNet V2-L implementation from rpi_testing_isef1_efficientnet_dog_emotion_classification.py, adopting its fine-tuning strategy (last 30 layers) and dense layer structure (2048, 1024 units).<br>\n",
        "Integrated the template’s (Cocker_Spaniel_Emotion_ResNet152_Training.ipynb) dataset structure, data splitting, and evaluation metrics (confusion matrix, classification report).<br>\n",
        "Ensured compatibility with Google Colab’s GPU environment, optimizing for the larger model’s computational demands.<br><br>"
      ],
      "metadata": {
        "id": "LGrjqwWBOjIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Notes:**<br>\n",
        "The code assumes an audio dataset at /content/drive/MyDrive/Cocker_Spaniel_Emotions_Audio with subfolders for each emotion (Sad, Happy, etc.) containing .wav files. Users must provide this dataset and update paths accordingly.<br>\n",
        "The Grad-CAM implementation uses the top_conv layer, the last convolutional layer in EfficientNetV2-L. Users should verify this using model.summary() if the architecture differs.<br>\n",
        "The input size is set to 480x480, optimal for EfficientNetV2-L, but may require significant GPU memory. Users with limited resources can reduce to 384x384, though this may slightly impact accuracy.<br>\n",
        "Training epochs are set to 50 (initial) + 15 (fine-tuning), with EarlyStopping to prevent overfitting. Users may adjust based on dataset size and convergence.\n",
        "The batch size is reduced to 16 to fit within Colab’s GPU memory for the larger model and input size. Users with high-end GPUs can increase it for faster training.<br>\n",
        "The TFLite model is optimized with float16 quantization, but EfficientNetV2-L’s size may challenge edge devices like Raspberry Pi 5. Users may need to test performance or consider model pruning for deployment.<br>"
      ],
      "metadata": {
        "id": "P_CNRp5qZ430"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "UPe9-av2PNVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cocker_Spaniel_Emotion_EfficientNetV2-L\n",
        "\n",
        "### Enhanced for 8-class Cocker Spaniel emotion classification using spectrogram images with EfficientNetV2-L for maximum accuracy."
      ],
      "metadata": {
        "id": "nV4Rl-1KPV0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Setup and Import Libraries\n",
        "# **Step 1**:"
      ],
      "metadata": {
        "id": "JNL_G7bNPctL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Step 1: Setup and Import Libraries\n",
        "# =============================\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import models, layers\n",
        "from tensorflow.keras.applications import EfficientNetV2L\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import librosa\n",
        "import librosa.display\n",
        "import gradio as gr\n",
        "from google.colab import drive\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "import pathlib\n",
        "import seaborn as sns\n",
        "import shutil\n",
        "import random"
      ],
      "metadata": {
        "id": "gCCj-w8JPMFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Enable GPU acceleration"
      ],
      "metadata": {
        "id": "PzD1s-C2Pg_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable GPU acceleration\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU available:\", tf.test.is_gpu_available())"
      ],
      "metadata": {
        "id": "tRYM--mmP9ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Mount Google Drive"
      ],
      "metadata": {
        "id": "w7wa8xjnPgUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mReB9z9YP5tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Define Dataset Paths and Convert Audio to Spectrogram Images\n",
        "# **Step 2**:"
      ],
      "metadata": {
        "id": "sSLwJfC4Phf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Step 2: Define Dataset Paths and Convert Audio to Spectrogram Images\n",
        "# =============================\n",
        "audio_dataset_dir = '/content/drive/MyDrive/Cocker_Spaniel_Emotions_Audio'\n",
        "spectrogram_dataset_dir = '/content/drive/MyDrive/Cocker_Spaniel_Emotions_Spectrogram'\n",
        "\n",
        "os.makedirs(spectrogram_dataset_dir, exist_ok=True)"
      ],
      "metadata": {
        "id": "Z_1FVdDiQBqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Define emotion classes"
      ],
      "metadata": {
        "id": "jSyEMlLjPiAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define emotion classes\n",
        "emotion_classes = [\"Sad\", \"Happy\", \"Stress\", \"Restless\", \"Normal\", \"Love\", \"Unhappy\", \"Tired\"]\n",
        "class_labels = {cls: idx for idx, cls in enumerate(emotion_classes)}"
      ],
      "metadata": {
        "id": "kiCEpXr4QFUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Convert audio files to spectrogram images"
      ],
      "metadata": {
        "id": "WVpvJ_b3Pi_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert audio files to spectrogram images\n",
        "def generate_spectrogram(audio_path, output_image_path):\n",
        "    \"\"\"Convert an audio file into a mel-spectrogram image.\"\"\"\n",
        "    y, sr = librosa.load(audio_path, sr=44100)\n",
        "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel')\n",
        "    plt.axis('off')\n",
        "    plt.savefig(output_image_path, bbox_inches='tight', pad_inches=0)\n",
        "    plt.close()"
      ],
      "metadata": {
        "id": "MOxsR_XOQJHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Process audio dataset"
      ],
      "metadata": {
        "id": "OIR8jH4DPjmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Process audio dataset\n",
        "for emotion in emotion_classes:\n",
        "    emotion_path = os.path.join(audio_dataset_dir, emotion)\n",
        "    output_emotion_folder = os.path.join(spectrogram_dataset_dir, emotion)\n",
        "    os.makedirs(output_emotion_folder, exist_ok=True)\n",
        "    if os.path.isdir(emotion_path):\n",
        "        for audio_file in os.listdir(emotion_path):\n",
        "            if audio_file.endswith('.wav'):\n",
        "                audio_path = os.path.join(emotion_path, audio_file)\n",
        "                output_image_path = os.path.join(output_emotion_folder, f\"{os.path.splitext(audio_file)[0]}.png\")\n",
        "                generate_spectrogram(audio_path, output_image_path)\n",
        "print(\"✅ Spectrogram dataset generated successfully.\")"
      ],
      "metadata": {
        "id": "3AidPRXNQNZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Count total spectrogram image"
      ],
      "metadata": {
        "id": "DMshQiRjPkQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count total spectrogram images\n",
        "dataset_dir = pathlib.Path(spectrogram_dataset_dir)\n",
        "total_images = len(list(dataset_dir.glob('*/*.png')))\n",
        "print(f\"Total spectrogram images in dataset: {total_images}\")"
      ],
      "metadata": {
        "id": "PJ9lKkaMQQ1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Count images per class"
      ],
      "metadata": {
        "id": "3t-EzeLePkzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count images per class\n",
        "image_counts = {}\n",
        "image_paths = {}\n",
        "for cls in emotion_classes:\n",
        "    count = len(list(dataset_dir.glob(f'{cls}/*')))\n",
        "    paths = list(dataset_dir.glob(f'{cls}/*'))\n",
        "    image_counts[cls] = count\n",
        "    image_paths[cls] = paths\n",
        "    print(f\"{cls}: {count} images\")"
      ],
      "metadata": {
        "id": "HVirfIB4QUe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Plot class distribution"
      ],
      "metadata": {
        "id": "rFdu-PxmPlfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot class distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(image_counts.keys(), image_counts.values(), color='skyblue')\n",
        "plt.xlabel('Emotion Class')\n",
        "plt.ylabel('Number of Spectrogram Images')\n",
        "plt.title('Distribution of Spectrogram Images by Emotion')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "siRC-InyQYMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Split Dataset into Train, Validation, and Test Sets\n",
        "# **Step 3**:"
      ],
      "metadata": {
        "id": "Ix0WQXj-PmDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Step 3: Split Dataset into Train, Validation, and Test Sets\n",
        "# =============================\n",
        "def split_dataset(dataset_dir, output_dir, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        os.makedirs(os.path.join(output_dir, split), exist_ok=True)\n",
        "\n",
        "    for cls in emotion_classes:\n",
        "        class_path = os.path.join(dataset_dir, cls)\n",
        "        images = os.listdir(class_path)\n",
        "        random.shuffle(images)\n",
        "\n",
        "        total_images = len(images)\n",
        "        train_end = int(train_ratio * total_images)\n",
        "        val_end = train_end + int(val_ratio * total_images)\n",
        "\n",
        "        train_images = images[:train_end]\n",
        "        val_images = images[train_end:val_end]\n",
        "        test_images = images[val_end:]\n",
        "\n",
        "        def copy_images(image_list, split):\n",
        "            split_class_dir = os.path.join(output_dir, split, cls)\n",
        "            os.makedirs(split_class_dir, exist_ok=True)\n",
        "            for img in image_list:\n",
        "                src = os.path.join(class_path, img)\n",
        "                dst = os.path.join(split_class_dir, img)\n",
        "                shutil.copy(src, dst)\n",
        "\n",
        "        copy_images(train_images, 'train')\n",
        "        copy_images(val_images, 'val')\n",
        "        copy_images(test_images, 'test')\n",
        "\n",
        "splitted_dataset_dir = '/content/drive/MyDrive/Cocker_Spaniel_Emotions_Splitted'\n",
        "split_dataset(dataset_dir, splitted_dataset_dir)\n",
        "print(\"✅ Dataset successfully split into training, validation, and testing sets!\")"
      ],
      "metadata": {
        "id": "JHjHCwYBQdDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Verify split counts"
      ],
      "metadata": {
        "id": "EMba2W3kPmYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify split counts\n",
        "for split in ['train', 'val', 'test']:\n",
        "    print(f\"\\n📂 {split.upper()} SET:\")\n",
        "    split_path = os.path.join(splitted_dataset_dir, split)\n",
        "    for cls in emotion_classes:\n",
        "        class_path = os.path.join(split_path, cls)\n",
        "        num_images = len(os.listdir(class_path)) if os.path.exists(class_path) else 0\n",
        "        print(f\"   - {cls}: {num_images} images\")"
      ],
      "metadata": {
        "id": "Vd_ojHhVQhWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Data Augmentation and Generators\n",
        "# **Step 4**:"
      ],
      "metadata": {
        "id": "EE-AcTLdPm1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Step 4: Data Augmentation and Generators\n",
        "# =============================\n",
        "IMG_HEIGHT, IMG_WIDTH, BATCH_SIZE = 480, 480, 16  # Increased input size for EfficientNetV2-L\n",
        "NUM_CLASSES = 8\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    fill_mode='nearest',\n",
        "    preprocessing_function=tf.keras.applications.efficientnet_v2.preprocess_input\n",
        ")\n",
        "\n",
        "val_test_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    preprocessing_function=tf.keras.applications.efficientnet_v2.preprocess_input\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    f'{splitted_dataset_dir}/train',\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_test_datagen.flow_from_directory(\n",
        "    f'{splitted_dataset_dir}/val',\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = val_test_datagen.flow_from_directory(\n",
        "    f'{splitted_dataset_dir}/test',\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(\"\\nClass indices:\", train_generator.class_indices)"
      ],
      "metadata": {
        "id": "eVeLSNv0Ql8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) Define EfficientNetV2-L Model\n",
        "# **Step 5**:"
      ],
      "metadata": {
        "id": "ezUbLwV9Pncy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Step 5: Define EfficientNetV2-L Model\n",
        "# =============================\n",
        "def create_efficientnetv2l_model(num_classes):\n",
        "    base_model = EfficientNetV2L(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
        "    base_model.trainable = False  # Freeze base model layers\n",
        "\n",
        "    inputs = base_model.input\n",
        "    x = base_model.output\n",
        "    x = layers.GlobalAveragePooling2D()(x)\n",
        "    x = layers.Dense(2048, activation='relu')(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "    x = layers.Dense(1024, activation='relu')(x)\n",
        "    x = layers.Dropout(0.3)(x)\n",
        "    outputs = layers.Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = models.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "model = create_efficientnetv2l_model(NUM_CLASSES)\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "RKcqGMkKQw8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Define callbacks"
      ],
      "metadata": {
        "id": "XSxBDBvtPoFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define callbacks\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
        "    '/content/drive/MyDrive/Cocker_Spaniel_Emotions/efficientnetv2l_spectrogram_best.h5',\n",
        "    monitor='val_accuracy', save_best_only=True\n",
        ")\n",
        "lr_scheduler = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)"
      ],
      "metadata": {
        "id": "5VCn72AtQ2MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Train the model"
      ],
      "metadata": {
        "id": "SkouDckLQtkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=50,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stopping, checkpoint, lr_scheduler]\n",
        ")"
      ],
      "metadata": {
        "id": "zmNryqY5Q5cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Fine-tune: Unfreeze last 30 layers"
      ],
      "metadata": {
        "id": "S9KTA0CrQt28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Fine-tune: Unfreeze last 30 layers\n",
        "base_model = model.layers[0]\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-30]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "fine_tune_epochs = 15\n",
        "history_fine = model.fit(\n",
        "    train_generator,\n",
        "    epochs=fine_tune_epochs,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stopping, checkpoint]\n",
        ")"
      ],
      "metadata": {
        "id": "b7LxohOLRA5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Evaluate on test set"
      ],
      "metadata": {
        "id": "emLT3OztQtvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test set\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(f\"\\n✅ Test Accuracy: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "mGCNA-aqQ852"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6) Plot Training Metrics\n",
        "# **Step 6**:"
      ],
      "metadata": {
        "id": "2gBqTgvbb1VG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Step 6: Plot Training Metrics\n",
        "# =============================\n",
        "def plot_training_metrics(history, history_fine=None):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "\n",
        "    if history_fine:\n",
        "        acc += history_fine.history['accuracy']\n",
        "        val_acc += history_fine.history['val_accuracy']\n",
        "        loss += history_fine.history['loss']\n",
        "        val_loss += history_fine.history['val_loss']\n",
        "\n",
        "    epochs_range = range(len(acc))\n",
        "\n",
        "    plt.figure(figsize=(14, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, acc, label='Training Accuracy', marker='o')\n",
        "    plt.plot(epochs_range, val_acc, label='Validation Accuracy', marker='x')\n",
        "    plt.title('📈 Training & Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, loss, label='Training Loss', marker='o', linestyle='--')\n",
        "    plt.plot(epochs_range, val_loss, label='Validation Loss', marker='x', linestyle='--')\n",
        "    plt.title('📉 Training & Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_metrics(history, history_fine)"
      ],
      "metadata": {
        "id": "HvyHYLypbza4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7) Confusion Matrix and Classification Report\n",
        "# **Step 7**:"
      ],
      "metadata": {
        "id": "5jtQ-R0MQt8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Step 7: Confusion Matrix and Classification Report\n",
        "# =============================\n",
        "def plot_confusion_matrix(model, test_generator):\n",
        "    class_names = list(test_generator.class_indices.keys())\n",
        "    y_pred_probs = model.predict(test_generator)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "    y_true = test_generator.classes\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Confusion Matrix 📊')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "plot_confusion_matrix(model, test_generator)"
      ],
      "metadata": {
        "id": "86MCkbGcRG0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 8) Grad-CAM for interpretability\n",
        "# **Step 8**:"
      ],
      "metadata": {
        "id": "FG6Lp2rsQt_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Step 8: Grad-CAM for Interpretability\n",
        "# =============================\n",
        "def get_gradcam_heatmap(model, img_array, last_conv_layer_name):\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
        "    )\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(img_array)\n",
        "        predicted_class = tf.argmax(predictions[0])\n",
        "        class_output = predictions[:, predicted_class]\n",
        "\n",
        "    grads = tape.gradient(class_output, conv_outputs)\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    conv_outputs = conv_outputs[0]\n",
        "    heatmap = tf.reduce_mean(tf.multiply(conv_outputs, pooled_grads), axis=-1)\n",
        "    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n",
        "    return heatmap\n",
        "\n",
        "def display_gradcam(img_path, model, last_conv_layer_name):\n",
        "    img = load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
        "    img_array = img_to_array(img)\n",
        "    img_array = tf.keras.applications.efficientnet_v2.preprocess_input(img_array)\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "\n",
        "    heatmap = get_gradcam_heatmap(model, img_array, last_conv_layer_name)\n",
        "    heatmap = cv2.resize(heatmap, (IMG_WIDTH, IMG_HEIGHT))\n",
        "\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "\n",
        "    superimposed_img = heatmap * 0.4 + img\n",
        "    superimposed_img = np.clip(superimposed_img, 0, 255).astype(np.uint8)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    plt.title('Original Spectrogram')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB))\n",
        "    plt.title('Grad-CAM Heatmap')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "pMRPpPw5RLSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Grad-CAM visualization"
      ],
      "metadata": {
        "id": "sL9TrzJyQuCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grad-CAM visualization\n",
        "img_path = '/content/drive/MyDrive/Cocker_Spaniel_Emotions_Splitted/val/Happy/sample_spec.png'  # Update with actual path\n",
        "display_gradcam(img_path, model, 'top_conv')  # Last conv layer in EfficientNetV2-L"
      ],
      "metadata": {
        "id": "MG2un-VQRQB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 9) Gradio Interface for Real-Time Prediction\n",
        "# **Step 9**:\n"
      ],
      "metadata": {
        "id": "9OeK8EfjcIjP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Step 9: Gradio Interface for Real-Time Prediction\n",
        "# =============================\n",
        "def predict_emotion(input_audio):\n",
        "    # Save uploaded audio to a temporary file\n",
        "    audio_path = \"temp_audio.wav\"\n",
        "    with open(audio_path, \"wb\") as f:\n",
        "        f.write(input_audio.read())\n",
        "\n",
        "    # Convert audio to spectrogram\n",
        "    spec_path = \"temp_spec.png\"\n",
        "    generate_spectrogram(audio_path, spec_path)\n",
        "\n",
        "    # Preprocess spectrogram image\n",
        "    spec_img = load_img(spec_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
        "    spec_array = img_to_array(spec_img)\n",
        "    spec_array = tf.keras.applications.efficientnet_v2.preprocess_input(spec_array)\n",
        "    spec_array = np.expand_dims(spec_array, axis=0)\n",
        "\n",
        "    # Predict emotion\n",
        "    pred = model.predict(spec_array)\n",
        "    emotion = emotion_classes[np.argmax(pred)]\n",
        "    confidence = np.max(pred) * 100\n",
        "\n",
        "    return f\"Predicted Emotion: {emotion} ({confidence:.2f}%)\"\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Cocker Spaniel Emotion Classification from Audio Spectrograms\")\n",
        "    audio_input = gr.Audio(label=\"Record/Upload Audio\", type=\"file\")\n",
        "    predict_button = gr.Button(\"Predict\")\n",
        "    output = gr.Textbox(label=\"Prediction\")\n",
        "\n",
        "    predict_button.click(predict_emotion, inputs=audio_input, outputs=output)\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "-UbxO0RXcMeY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 10) Convert to TensorFlow Lite\n",
        "# **Step 10**:"
      ],
      "metadata": {
        "id": "3Uig21LycQ6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Step 10: Convert to TensorFlow Lite\n",
        "# =============================\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "tflite_model = converter.convert()\n",
        "with open('/content/drive/MyDrive/Cocker_Spaniel_Emotions/efficientnetv2l_spectrogram.tflite', 'wb') as f:\n",
        "    f.write(tflite_model)\n",
        "print(\"✅ TFLite model saved successfully.\")"
      ],
      "metadata": {
        "id": "SDe86nHwcPrT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Save the EfficientNetV2-L model"
      ],
      "metadata": {
        "id": "o2QQPwOnQuF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the full EfficientNetV2-L model\n",
        "model.save('/content/drive/MyDrive/Cocker_Spaniel_Emotions/efficientnetv2l_spectrogram_final.h5')\n",
        "print(\"✅ Model saved successfully as 'efficientnetv2l_spectrogram_final.h5'\")"
      ],
      "metadata": {
        "id": "UY7jIoh4RS5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Test prediction\n",
        "## Test prediction on a single image"
      ],
      "metadata": {
        "id": "_bgd_L-pQuI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test prediction on a single image\n",
        "def predict_single_image(img_path, model):\n",
        "    img = image.load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
        "\n",
        "    prediction = model.predict(img_array)\n",
        "    predicted_class = emotion_classes[np.argmax(prediction)]\n",
        "\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Predicted Emotion: {predicted_class}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "predict_single_image(img_path, model)"
      ],
      "metadata": {
        "id": "C1kzbOlTRbkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "---\n"
      ],
      "metadata": {
        "id": "j_1SC8RuRgcK"
      }
    }
  ]
}