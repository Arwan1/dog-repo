{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/radhakrishnan-omotec/arwan-iris-dog-repo/blob/main/ISEF_FINAL_ArwanMakhija_Prediction_Resnet152_CNN_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "qMquMI4EPOei"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Enhanced Python Notebook for **TailSense** : ResNet152-based Canine Pet Image and Audio Spectrogram Classification\n",
        "\n",
        "### Author : ARWAN MAKHIJA"
      ],
      "metadata": {
        "id": "vAkb_3X7TGFx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is an enhanced Python notebook implementation for Google Colab that integrates both image classification and spectrogram audio classification using the ResNet152 model, optimized for maximum accuracy and depth.\n",
        "\n",
        "It leverages ResNet152â€™s deep architecture (~60M parameters) with residual connections for classifying dog emotions from both image and audio data derived from videos of a Cocker Spaniel.\n",
        "\n",
        "The dataset is assumed to contain 8-10 emotion classes (e.g., \"defensive,\" \"stressed,\" \"friendly\"), and the implementation includes data preprocessing, model training, evaluation, a Gradio interface for real-time inference, and TensorFlow Lite conversion for edge deployment."
      ],
      "metadata": {
        "id": "OKtmg6QfTHE_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ResNet152-based Image Classification"
      ],
      "metadata": {
        "id": "hm0a5UB_TJ7O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 1** : Cocker Spaniel 8 Emotions Prediction Resnet152 CNN Model Traning\n"
      ],
      "metadata": {
        "id": "E-1xViExRpra"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "m8lvEkuETUTk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# My Project Key Enhancements:\n",
        "**8-Class Cocker Spaniel Emotion Classifier:**<br>\n",
        "Replaced the 3-class tongue classification (Healthy Tongue, Tongue with Moderate Coating, Tongue with Severe Coating) with an 8-class emotion classification (Sad, Happy, Stress, Restless, Normal, Love, Unhappy, Tired).\n",
        "Updated dataset paths to a hypothetical Cocker Spaniel emotion dataset, with class folders named after emotions.<br><br>\n",
        "Modified class labels, image counting, and visualization to handle 8 classes, ensuring compatibility with the new dataset structure.<br><br>\n",
        "Adjusted the confusion matrix and classification report to reflect the increased number of classes, with a larger figure size for clarity.<br><br><br>\n",
        "\n",
        "**ResNet152-Based Transfer-Learning Model:**<br>\n",
        "Replaced the Sequential CNN with a pre-trained ResNet152 model from tensorflow.keras.applications, using ImageNet weights for transfer learning.\n",
        "Froze the base model layers (base_model.trainable = False) to leverage pre-trained features and reduce training time.<br><br>\n",
        "Added custom layers: GlobalAveragePooling2D for spatial dimension reduction, two dense layers (512 and 256 units) with ReLU activation, and dropout layers (0.5 and 0.3) to prevent overfitting. The final dense layer outputs probabilities for 8 classes with softmax activation.<br><br>\n",
        "Updated the Grad-CAM function to use the last convolutional layer of ResNet152 (conv5_block3_3_conv), ensuring compatibility with the new model architecture.<br>"
      ],
      "metadata": {
        "id": "LGrjqwWBOjIv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "UPe9-av2PNVH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ISEF Reviewer Notes:**<br>\n",
        "The code assumes the existence of a Cocker Spaniel emotion dataset at /content/drive/MyDrive/Cocker_Spaniel_Emotions_Dataset.zip, with subfolders named after each emotion (Sad, Happy, etc.). Users must provide this dataset and update paths accordingly.<br><br>\n",
        "The Grad-CAM implementation uses the conv5_block3_3_conv layer, which is the last convolutional layer in ResNet152. Users should verify this using model.summary() if the architecture differs.<br><br>\n",
        "The notebook is designed for Google Colab with GPU acceleration, as specified in the original metadata.<br><br>\n",
        "The code supports .jpg and .png formats, accommodating common image types in emotion datasets.<br><br>\n",
        "Training epochs are set to 50, with EarlyStopping to prevent overfitting; users may adjust based on dataset size and convergence behavior.<br>"
      ],
      "metadata": {
        "id": "7Nxm3MBiO4-x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "xt8J54wGPP68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Cocker_Spaniel_Emotion_ResNet152\n",
        "\n",
        "### Enhanced for 8-class Cocker Spaniel emotion prediction using ResNet152 with transfer learning."
      ],
      "metadata": {
        "id": "nV4Rl-1KPV0W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Import necessary libraries"
      ],
      "metadata": {
        "id": "JNL_G7bNPctL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from google.colab import drive\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import shutil\n",
        "import random\n",
        "import pathlib\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import ResNet152\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "from tensorflow.keras.preprocessing import image\n",
        "import cv2\n",
        "import PIL"
      ],
      "metadata": {
        "id": "gCCj-w8JPMFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Mount Google Drive"
      ],
      "metadata": {
        "id": "w7wa8xjnPgUJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "mReB9z9YP5tI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Check TensorFlow version"
      ],
      "metadata": {
        "id": "PzD1s-C2Pg_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check TensorFlow version\n",
        "print(f\"TensorFlow Version: {tf.__version__}\")"
      ],
      "metadata": {
        "id": "tRYM--mmP9ca"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Unzip the dataset (assumed to be a Cocker Spaniel emotion dataset)"
      ],
      "metadata": {
        "id": "sSLwJfC4Phf2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Unzip the dataset (assumed to be a Cocker Spaniel emotion dataset)\n",
        "!unzip \"/content/drive/MyDrive/Cocker_Spaniel_Emotions_Dataset.zip\" -d \"/content/drive/MyDrive/Cocker_Spaniel_Emotions_Dataset\""
      ],
      "metadata": {
        "id": "Z_1FVdDiQBqo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Define dataset directory"
      ],
      "metadata": {
        "id": "jSyEMlLjPiAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define dataset directory\n",
        "dataset_dir = \"/content/drive/MyDrive/Cocker_Spaniel_Emotions_Dataset/Cocker_Spaniel_Emotions_Dataset\"\n",
        "dataset_dir = pathlib.Path(dataset_dir)"
      ],
      "metadata": {
        "id": "kiCEpXr4QFUj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Count total images"
      ],
      "metadata": {
        "id": "WVpvJ_b3Pi_w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count total images\n",
        "total_images = len(list(dataset_dir.glob('*/*.jpg'))) + len(list(dataset_dir.glob('*/*.png')))\n",
        "print(f\"Total images in dataset: {total_images}\")"
      ],
      "metadata": {
        "id": "MOxsR_XOQJHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Define emotion classes and labels"
      ],
      "metadata": {
        "id": "OIR8jH4DPjmt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define emotion classes and labels\n",
        "emotion_classes = [\"Sad\", \"Happy\", \"Stress\", \"Restless\", \"Normal\", \"Love\", \"Unhappy\", \"Tired\"]\n",
        "class_labels = {cls: idx for idx, cls in enumerate(emotion_classes)}\n",
        "image_counts = {}\n",
        "image_paths = {}"
      ],
      "metadata": {
        "id": "3AidPRXNQNZs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Count images per class"
      ],
      "metadata": {
        "id": "DMshQiRjPkQG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Count images per class\n",
        "for cls in emotion_classes:\n",
        "    count = len(list(dataset_dir.glob(f'{cls}/*')))\n",
        "    paths = list(dataset_dir.glob(f'{cls}/*'))\n",
        "    image_counts[cls] = count\n",
        "    image_paths[cls] = paths\n",
        "    print(f\"{cls}: {count} images\")"
      ],
      "metadata": {
        "id": "PJ9lKkaMQQ1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Visualize sample images"
      ],
      "metadata": {
        "id": "3t-EzeLePkzf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualize sample images\n",
        "for cls, paths in image_paths.items():\n",
        "    if paths:\n",
        "        print(f\"\\nSample {cls} Image:\")\n",
        "        PIL.Image.open(str(paths[min(10, len(paths)-1)])).show()"
      ],
      "metadata": {
        "id": "HVirfIB4QUe3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Plot class distribution"
      ],
      "metadata": {
        "id": "rFdu-PxmPlfO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot class distribution\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.bar(image_counts.keys(), image_counts.values(), color='skyblue')\n",
        "plt.xlabel('Emotion Class')\n",
        "plt.ylabel('Number of Images')\n",
        "plt.title('Distribution of Cocker Spaniel Emotion Images')\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "siRC-InyQYMb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Split dataset into train, validation, and test sets"
      ],
      "metadata": {
        "id": "Ix0WQXj-PmDp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split dataset into train, validation, and test sets\n",
        "def split_dataset(dataset_dir, output_dir, train_ratio=0.7, val_ratio=0.15, test_ratio=0.15):\n",
        "    for split in ['train', 'val', 'test']:\n",
        "        os.makedirs(os.path.join(output_dir, split), exist_ok=True)\n",
        "\n",
        "    for cls in emotion_classes:\n",
        "        class_path = os.path.join(dataset_dir, cls)\n",
        "        images = os.listdir(class_path)\n",
        "        random.shuffle(images)\n",
        "\n",
        "        total_images = len(images)\n",
        "        train_end = int(train_ratio * total_images)\n",
        "        val_end = train_end + int(val_ratio * total_images)\n",
        "\n",
        "        train_images = images[:train_end]\n",
        "        val_images = images[train_end:val_end]\n",
        "        test_images = images[val_end:]\n",
        "\n",
        "        def copy_images(image_list, split):\n",
        "            split_class_dir = os.path.join(output_dir, split, cls)\n",
        "            os.makedirs(split_class_dir, exist_ok=True)\n",
        "            for img in image_list:\n",
        "                src = os.path.join(class_path, img)\n",
        "                dst = os.path.join(split_class_dir, img)\n",
        "                shutil.copy(src, dst)\n",
        "\n",
        "        copy_images(train_images, 'train')\n",
        "        copy_images(val_images, 'val')\n",
        "        copy_images(test_images, 'test')\n",
        "\n",
        "splitted_dataset_dir = '/content/drive/MyDrive/Cocker_Spaniel_Emotions_Splitted'\n",
        "split_dataset(dataset_dir, splitted_dataset_dir)\n",
        "print(\"âœ… Dataset successfully split into training, validation, and testing sets!\")"
      ],
      "metadata": {
        "id": "JHjHCwYBQdDO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Verify split counts"
      ],
      "metadata": {
        "id": "EMba2W3kPmYy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Verify split counts\n",
        "for split in ['train', 'val', 'test']:\n",
        "    print(f\"\\nðŸ“‚ {split.upper()} SET:\")\n",
        "    split_path = os.path.join(splitted_dataset_dir, split)\n",
        "    for cls in emotion_classes:\n",
        "        class_path = os.path.join(split_path, cls)\n",
        "        num_images = len(os.listdir(class_path)) if os.path.exists(class_path) else 0\n",
        "        print(f\"   - {cls}: {num_images} images\")"
      ],
      "metadata": {
        "id": "Vd_ojHhVQhWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Data augmentation and generators"
      ],
      "metadata": {
        "id": "EE-AcTLdPm1A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Data augmentation and generators\n",
        "IMG_HEIGHT, IMG_WIDTH, BATCH_SIZE = 224, 224, 32\n",
        "NUM_CLASSES = 8\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=30,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.3,\n",
        "    horizontal_flip=True,\n",
        "    vertical_flip=True,\n",
        "    brightness_range=[0.8, 1.2],\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "\n",
        "val_test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    f'{splitted_dataset_dir}/train',\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "val_generator = val_test_datagen.flow_from_directory(\n",
        "    f'{splitted_dataset_dir}/val',\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "\n",
        "test_generator = val_test_datagen.flow_from_directory(\n",
        "    f'{splitted_dataset_dir}/test',\n",
        "    target_size=(IMG_HEIGHT, IMG_WIDTH),\n",
        "    batch_size=BATCH_SIZE,\n",
        "    class_mode='categorical',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "print(\"\\nClass indices:\", train_generator.class_indices)"
      ],
      "metadata": {
        "id": "eVeLSNv0Ql8h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Define ResNet152-based model"
      ],
      "metadata": {
        "id": "ezUbLwV9Pncy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define ResNet152-based model\n",
        "def create_resnet152_model():\n",
        "    base_model = ResNet152(weights='imagenet', include_top=False, input_shape=(IMG_HEIGHT, IMG_WIDTH, 3))\n",
        "    base_model.trainable = False  # Freeze base model layers\n",
        "\n",
        "    inputs = base_model.input\n",
        "    x = base_model.output\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    x = Dense(512, activation='relu')(x)\n",
        "    x = Dropout(0.5)(x)\n",
        "    x = Dense(256, activation='relu')(x)\n",
        "    x = Dropout(0.3)(x)\n",
        "    outputs = Dense(NUM_CLASSES, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n",
        "\n",
        "model = create_resnet152_model()\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "RKcqGMkKQw8s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Define callbacks"
      ],
      "metadata": {
        "id": "XSxBDBvtPoFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define callbacks\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "checkpoint = ModelCheckpoint('/content/drive/MyDrive/Cocker_Spaniel_Emotions/emotion_model_best.h5',\n",
        "                            monitor='val_accuracy', save_best_only=True)\n",
        "lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n"
      ],
      "metadata": {
        "id": "5VCn72AtQ2MG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Train the model"
      ],
      "metadata": {
        "id": "SkouDckLQtkN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "history = model.fit(\n",
        "    train_generator,\n",
        "    epochs=50,\n",
        "    validation_data=val_generator,\n",
        "    callbacks=[early_stopping, checkpoint, lr_scheduler]\n",
        ")"
      ],
      "metadata": {
        "id": "zmNryqY5Q5cd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Evaluate on test set"
      ],
      "metadata": {
        "id": "emLT3OztQtvz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate on test set\n",
        "test_loss, test_accuracy = model.evaluate(test_generator)\n",
        "print(f\"\\nâœ… Test Accuracy: {test_accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "id": "mGCNA-aqQ852"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Plot training and validation metrics"
      ],
      "metadata": {
        "id": "S9KTA0CrQt28"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot training and validation metrics\n",
        "def plot_training_metrics(history):\n",
        "    acc = history.history['accuracy']\n",
        "    val_acc = history.history['val_accuracy']\n",
        "    loss = history.history['loss']\n",
        "    val_loss = history.history['val_loss']\n",
        "    epochs_range = range(len(acc))\n",
        "\n",
        "    plt.figure(figsize=(14, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(epochs_range, acc, label='Training Accuracy', marker='o')\n",
        "    plt.plot(epochs_range, val_acc, label='Validation Accuracy', marker='x')\n",
        "    plt.title('ðŸ“ˆ Training & Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(epochs_range, loss, label='Training Loss', marker='o', linestyle='--')\n",
        "    plt.plot(epochs_range, val_loss, label='Validation Loss', marker='x', linestyle='--')\n",
        "    plt.title('ðŸ“‰ Training & Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "plot_training_metrics(history)"
      ],
      "metadata": {
        "id": "b7LxohOLRA5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Confusion matrix and classification report"
      ],
      "metadata": {
        "id": "5jtQ-R0MQt8v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix and classification report\n",
        "def plot_confusion_matrix(model, test_generator):\n",
        "    class_names = list(test_generator.class_indices.keys())\n",
        "    y_pred_probs = model.predict(test_generator)\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
        "    y_true = test_generator.classes\n",
        "\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
        "    plt.title('Confusion Matrix ðŸ“Š')\n",
        "    plt.xlabel('Predicted Labels')\n",
        "    plt.ylabel('True Labels')\n",
        "    plt.xticks(rotation=45, ha='right')\n",
        "    plt.show()\n",
        "\n",
        "    print(\"Classification Report:\")\n",
        "    print(classification_report(y_true, y_pred, target_names=class_names))\n",
        "\n",
        "plot_confusion_matrix(model, test_generator)"
      ],
      "metadata": {
        "id": "86MCkbGcRG0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Grad-CAM for interpretability"
      ],
      "metadata": {
        "id": "FG6Lp2rsQt_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grad-CAM for interpretability\n",
        "def get_gradcam_heatmap(model, img_array, last_conv_layer_name):\n",
        "    grad_model = tf.keras.models.Model(\n",
        "        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n",
        "    )\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        conv_outputs, predictions = grad_model(img_array)\n",
        "        predicted_class = tf.argmax(predictions[0])\n",
        "        class_output = predictions[:, predicted_class]\n",
        "\n",
        "    grads = tape.gradient(class_output, conv_outputs)\n",
        "    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
        "\n",
        "    conv_outputs = conv_outputs[0]\n",
        "    heatmap = tf.reduce_mean(tf.multiply(conv_outputs, pooled_grads), axis=-1)\n",
        "    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n",
        "    return heatmap\n",
        "\n",
        "def display_gradcam(img_path, model, last_conv_layer_name):\n",
        "    img = image.load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
        "\n",
        "    heatmap = get_gradcam_heatmap(model, img_array, last_conv_layer_name)\n",
        "    heatmap = cv2.resize(heatmap, (IMG_WIDTH, IMG_HEIGHT))\n",
        "\n",
        "    img = cv2.imread(img_path)\n",
        "    img = cv2.resize(img, (IMG_WIDTH, IMG_HEIGHT))\n",
        "    heatmap = np.uint8(255 * heatmap)\n",
        "    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n",
        "\n",
        "    superimposed_img = heatmap * 0.4 + img\n",
        "    superimposed_img = np.clip(superimposed_img, 0, 255).astype(np.uint8)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.imshow(cv2.cvtColor(img, cv2.COLOR_BGR2RGB))\n",
        "    plt.title('Original Image')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.imshow(cv2.cvtColor(superimposed_img, cv2.COLOR_BGR2RGB))\n",
        "    plt.title('Grad-CAM Heatmap')\n",
        "    plt.axis('off')\n",
        "\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "pMRPpPw5RLSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Grad-CAM visualization"
      ],
      "metadata": {
        "id": "sL9TrzJyQuCP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Grad-CAM visualization\n",
        "img_path = '/content/drive/MyDrive/Cocker_Spaniel_Emotions_Splitted/val/Happy/sample_image.jpg'  # Update with actual path\n",
        "display_gradcam(img_path, model, 'conv5_block3_3_conv')  # Last conv layer in ResNet152"
      ],
      "metadata": {
        "id": "MG2un-VQRQB_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Save the Resnet152 model"
      ],
      "metadata": {
        "id": "o2QQPwOnQuF9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the Resnet152 model\n",
        "model.save('/content/drive/MyDrive/Cocker_Spaniel_Emotions/emotion_detection_resnet152.h5')\n",
        "print(\"âœ… Model saved successfully as 'emotion_detection_resnet152.h5'\")"
      ],
      "metadata": {
        "id": "UY7jIoh4RS5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Test prediction\n",
        "## Test prediction on a single image"
      ],
      "metadata": {
        "id": "_bgd_L-pQuI2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Test prediction on a single image\n",
        "def predict_single_image(img_path, model):\n",
        "    img = image.load_img(img_path, target_size=(IMG_HEIGHT, IMG_WIDTH))\n",
        "    img_array = image.img_to_array(img)\n",
        "    img_array = np.expand_dims(img_array, axis=0) / 255.0\n",
        "\n",
        "    prediction = model.predict(img_array)\n",
        "    predicted_class = emotion_classes[np.argmax(prediction)]\n",
        "\n",
        "    plt.imshow(img)\n",
        "    plt.title(f\"Predicted Emotion: {predicted_class}\")\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "predict_single_image(img_path, model)"
      ],
      "metadata": {
        "id": "C1kzbOlTRbkj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---\n",
        "---\n"
      ],
      "metadata": {
        "id": "j_1SC8RuRgcK"
      }
    }
  ]
}