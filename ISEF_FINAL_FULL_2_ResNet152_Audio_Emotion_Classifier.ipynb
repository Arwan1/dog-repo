{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# \ud83c\udfa7 ResNet152 Audio Emotion Classification\n",
        "\n",
        "This notebook trains a CNN model using **ResNet152** on **audio spectrograms** for emotion classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83d\udce6 Step 1: Install required libraries\n",
        "!pip install librosa matplotlib scikit-learn tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83d\udcda Step 2: Import libraries\n",
        "import os\n",
        "import librosa\n",
        "import librosa.display\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.applications import ResNet152\n",
        "from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from PIL import Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \u2699\ufe0f Step 3: Configuration\n",
        "AUDIO_DIR = \"/content/audio_data\"  # Upload your dataset here\n",
        "SAMPLE_RATE = 22050\n",
        "DURATION = 2\n",
        "N_MELS = 128\n",
        "IMG_SIZE = 224\n",
        "CLASSES = ['Sad', 'Happy', 'Stress', 'Restless', 'Love', 'Lonely', 'Tired', 'Normal']\n",
        "CLASS_TO_IDX = {label: idx for idx, label in enumerate(CLASSES)}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83c\udfbc Step 4: Convert audio to spectrogram\n",
        "def audio_to_spectrogram(file_path):\n",
        "    y, sr = librosa.load(file_path, duration=DURATION, sr=SAMPLE_RATE)\n",
        "    mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=N_MELS)\n",
        "    mel_db = librosa.power_to_db(mel, ref=np.max)\n",
        "    img = librosa.util.normalize(mel_db)\n",
        "    img = Image.fromarray(np.uint8(plt.cm.viridis(img) * 255))\n",
        "    img = img.resize((IMG_SIZE, IMG_SIZE)).convert(\"RGB\")\n",
        "    return np.array(img)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83d\udcc1 Step 5: Load dataset into X, y\n",
        "X, y = [], []\n",
        "\n",
        "for label in CLASSES:\n",
        "    folder = os.path.join(AUDIO_DIR, label)\n",
        "    for file in os.listdir(folder):\n",
        "        try:\n",
        "            img = audio_to_spectrogram(os.path.join(folder, file))\n",
        "            X.append(img)\n",
        "            y.append(CLASS_TO_IDX[label])\n",
        "        except Exception as e:\n",
        "            print(f\"Skipped {file} due to {e}\")\n",
        "\n",
        "X = np.array(X) / 255.0\n",
        "y = to_categorical(y, num_classes=len(CLASSES))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \u2702\ufe0f Step 6: Split data\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=y, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83e\udde0 Step 7: Create model using ResNet152\n",
        "base_model = ResNet152(weights='imagenet', include_top=False, input_shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "base_model.trainable = False\n",
        "\n",
        "x = GlobalAveragePooling2D()(base_model.output)\n",
        "x = Dropout(0.3)(x)\n",
        "x = Dense(256, activation='relu')(x)\n",
        "x = Dropout(0.3)(x)\n",
        "output = Dense(len(CLASSES), activation='softmax')(x)\n",
        "\n",
        "model = Model(inputs=base_model.input, outputs=output)\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83c\udfcb\ufe0f\u200d\u2642\ufe0f Step 8: Train model\n",
        "datagen = ImageDataGenerator(horizontal_flip=True)\n",
        "history = model.fit(datagen.flow(X_train, y_train, batch_size=16),\n",
        "                    validation_data=(X_val, y_val),\n",
        "                    epochs=15)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# \ud83d\udcbe Step 9: Save model\n",
        "model.save(\"resnet_audio_emotion_model.h5\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": ""
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}