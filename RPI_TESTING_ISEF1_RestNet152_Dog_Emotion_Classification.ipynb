{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/radhakrishnan-omotec/arwan-iris-dog-repo/blob/main/RPI_TESTING_ISEF1_RestNet152_Dog_Emotion_Classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zJKfFiwImizW"
      },
      "source": [
        "# Enhanced Python Notebook for **TailSense** : ResNet152-based Canine Pet Image and Audio Spectrogram Classification\n",
        "\n",
        "### Author : ARWAN MAKHIJA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PG9fzi1Ymizm"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Below is an enhanced Python notebook implementation for Google Colab that integrates both image classification and spectrogram audio classification using the ResNet152 model, optimized for maximum accuracy and depth.\n",
        "\n",
        "It leverages ResNet152’s deep architecture (~60M parameters) with residual connections for classifying dog emotions from both image and audio data derived from videos of a Cocker Spaniel.\n",
        "\n",
        "The dataset is assumed to contain 8-10 emotion classes (e.g., \"defensive,\" \"stressed,\" \"friendly\"), and the implementation includes data preprocessing, model training, evaluation, a Gradio interface for real-time inference, and TensorFlow Lite conversion for edge deployment."
      ],
      "metadata": {
        "id": "uzX2lnt59b82"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "zRkMe5VP9vae"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HUciruA1mizm"
      },
      "source": [
        "# ResNet152-based Image and Spectrogram Audio Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This notebook implements a dual-classification system using the ResNet152 Convolutional Neural Network (CNN) for classifying dog emotions from both image data (extracted from video frames) and spectrogram audio data (derived from video audio).\n",
        "\n",
        "The system is optimized for Colab’s GPU environment and includes:\n",
        "\n",
        "**Image Classification**: Extracts frames from videos and classifies them into 8-10 emotion categories using ResNet152. <br>\n",
        "**Spectrogram Audio Classification**: Converts audio segments into mel-spectrogram images and classifies them using a separate ResNet152 model.<br>\n",
        "**Real-time Inference**: Provides a Gradio interface for capturing and predicting emotions from image and audio inputs.<br>\n",
        "**Edge Deployment**: Converts models to TensorFlow Lite for deployment on devices like Raspberry Pi 5.<br><br><br>\n",
        "The goal is to achieve the highest possible accuracy (targeting 92-95%) by leveraging ResNet152’s depth, fine-tuning, and data augmentation."
      ],
      "metadata": {
        "id": "RfstPdgu9wKP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workflow <br>\n",
        "Setup and Import Libraries<br>\n",
        "\n",
        "1.   Setup and Import Libraries<br>\n",
        "2.   Generate Individual IMAGES Dataset from Videos<br>\n",
        "3.   Train ResNet152 Model on IMAGES Dataset for Emotion Classification<br>\n",
        "4.   Split Audio into AUDIO Dataset Folders<br>\n",
        "5.   Convert Audio to SPECTROGRAM Images<br>\n",
        "6.   Train ResNet152 Model on AUDIO SPECTROGRAM Dataset for Emotion Classification<br>\n",
        "7.   Develop Gradio User Interface for Input Capture (Image and Audio Spectrogram)<br>\n",
        "8.   Integrate the Gradio Interface with Raspberry Pi 5<br>\n",
        "9.   Predict Emotions Using CNN Models via Gradio Interface<br>\n",
        "10.   Evaluate Results with Confusion Matrix and Metrics + Convert to TensorFlow Lite<br>\n",
        "11.   Evaluate and Visualize Model Results<br>\n",
        "12.   Develop Gradio Interface for Real-Time Input Capture and Prediction<br>\n",
        "13.   Raspberry Pi Integration Snippet [OPTIONAL] <br>\n"
      ],
      "metadata": {
        "id": "_xDRoiU5-L1d"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVgAyse_mizt"
      },
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 1: Setup and Import Libraries**"
      ],
      "metadata": {
        "id": "LRYqNwbV_SCg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "Imports libraries for deep learning (TensorFlow, Keras), image processing (OpenCV), audio processing (Librosa), and user interface (Gradio).\n",
        "Enables GPU support in Google Colab for faster training and inference."
      ],
      "metadata": {
        "id": "w4QVrus6_hR2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell : Setup and Imports\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.applications import ResNet152\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import cv2\n",
        "import librosa\n",
        "import librosa.display\n",
        "import gradio as gr\n",
        "from google.colab import drive\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import zipfile"
      ],
      "metadata": {
        "id": "N008CQhs_fg1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Enable GPU\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "if physical_devices:\n",
        "    tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "print(\"TensorFlow version:\", tf.__version__)\n",
        "print(\"GPU available:\", tf.test.is_gpu_available())"
      ],
      "metadata": {
        "id": "AgyIeQVmD82z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 2: Generate Individual IMAGES Dataset from Videos**"
      ],
      "metadata": {
        "id": "QKPkH8Ty_VdE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "Mounts Google Drive to access the video dataset.\n",
        "Extracts frames from videos at a rate of 1 frame per second and organizes them into emotion-specific subfolders within IMAGE_DATASET."
      ],
      "metadata": {
        "id": "NYmLsFUb_pyv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell : Extract Frames from Videos\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define paths\n",
        "video_dir = '/content/drive/MyDrive/Arwan IRIS/DATASET_VIDEOS'\n",
        "image_dataset_dir = '/content/drive/MyDrive/Arwan IRIS/IMAGE_DATASET'\n",
        "\n"
      ],
      "metadata": {
        "id": "hxO28I2O_m5A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create image dataset directory if it doesn't exist\n",
        "os.makedirs(image_dataset_dir, exist_ok=True)\n"
      ],
      "metadata": {
        "id": "Z3WU3VnKEmU0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract frames from videos\n",
        "def extract_frames(video_path, output_folder, frame_rate=1):\n",
        "    vidcap = cv2.VideoCapture(video_path)\n",
        "    success, image = vidcap.read()\n",
        "    count = 0\n",
        "    while success:\n",
        "        if count % frame_rate == 0:\n",
        "            cv2.imwrite(os.path.join(output_folder, f\"frame{count}.jpg\"), image)\n",
        "        success, image = vidcap.read()\n",
        "        count += 1"
      ],
      "metadata": {
        "id": "dmMgR9HgEou8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process each emotion folder\n",
        "for emotion_folder in os.listdir(video_dir):\n",
        "    emotion_path = os.path.join(video_dir, emotion_folder)\n",
        "    if os.path.isdir(emotion_path):\n",
        "        output_emotion_folder = os.path.join(image_dataset_dir, emotion_folder)\n",
        "        os.makedirs(output_emotion_folder, exist_ok=True)\n",
        "        for video_file in os.listdir(emotion_path):\n",
        "            if video_file.endswith('.mp4'):\n",
        "                video_path = os.path.join(emotion_path, video_file)\n",
        "                extract_frames(video_path, output_emotion_folder)\n",
        "\n",
        "print(\"Image dataset generated successfully.\")"
      ],
      "metadata": {
        "id": "lxaUYhtkEqNT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 3: Train ResNet152 Model on IMAGES Dataset for Emotion Classification**"
      ],
      "metadata": {
        "id": "M9WkmMwn_WMr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "Preprocesses images with augmentation to improve generalization.\n",
        "Defines a ResNet152 model with a custom head (2048 and 1024 dense layers) for image classification.\n",
        "Trains initially with frozen base layers, then fine-tunes the last 30 layers for higher accuracy."
      ],
      "metadata": {
        "id": "ZucNFDkN_yT-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell : Load and Preprocess Image Dataset\n",
        "img_height, img_width = 224, 224\n",
        "batch_size = 32\n",
        "num_classes = len(os.listdir(image_dataset_dir))  # Number of emotion classes\n",
        "\n",
        "train_datagen = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2,\n",
        "    preprocessing_function=tf.keras.applications.resnet.preprocess_input\n",
        ")\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    image_dataset_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator = train_datagen.flow_from_directory(\n",
        "    image_dataset_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "class_names = list(train_generator.class_indices.keys())\n",
        "print(\"Class names:\", class_names)"
      ],
      "metadata": {
        "id": "C4KKLW0m_wy4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell : Define and Train ResNet152 Model for Images\n",
        "def create_resnet152_model(num_classes):\n",
        "    base_model = ResNet152(weights='imagenet', include_top=False, input_shape=(224, 224, 3))\n",
        "    base_model.trainable = False\n",
        "    model = models.Sequential([\n",
        "        base_model,\n",
        "        layers.GlobalAveragePooling2D(),\n",
        "        layers.Dense(2048, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(1024, activation='relu'),\n",
        "        layers.Dropout(0.5),\n",
        "        layers.Dense(num_classes, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model_image = create_resnet152_model(num_classes)\n",
        "model_image.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "epochs = 20\n",
        "callbacks = [\n",
        "    tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
        "    tf.keras.callbacks.ModelCheckpoint('/content/drive/MyDrive/Arwan IRIS/resnet152_image_best.h5',\n",
        "                                       monitor='val_accuracy', save_best_only=True)\n",
        "]\n",
        "\n",
        "history_image = model_image.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=val_generator.samples // batch_size,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# Fine-tune last 30 layers\n",
        "base_model = model_image.layers[0]\n",
        "base_model.trainable = True\n",
        "for layer in base_model.layers[:-30]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model_image.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "fine_tune_epochs = 10\n",
        "history_fine_image = model_image.fit(\n",
        "    train_generator,\n",
        "    steps_per_epoch=train_generator.samples // batch_size,\n",
        "    validation_data=val_generator,\n",
        "    validation_steps=val_generator.samples // batch_size,\n",
        "    epochs=fine_tune_epochs,\n",
        "    callbacks=callbacks\n",
        ")\n"
      ],
      "metadata": {
        "id": "hUFiY6dh_1se"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_image.save('/content/drive/MyDrive/Arwan IRIS/resnet152_image_final.h5')"
      ],
      "metadata": {
        "id": "LfF7Q_q-Evsg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 4: Split Audio into AUDIO Dataset Folders**"
      ],
      "metadata": {
        "id": "vOOZhe_m_Wqr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "Extracts audio from videos using FFmpeg and splits it into 10-second segments.\n",
        "Saves segments into emotion-specific subfolders within AUDIO_DATASET."
      ],
      "metadata": {
        "id": "DiSctUIgAATr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell : Extract and Split Audio from Videos\n",
        "audio_dataset_dir = '/content/drive/MyDrive/Arwan IRIS/AUDIO_DATASET'\n",
        "os.makedirs(audio_dataset_dir, exist_ok=True)\n",
        "\n",
        "def extract_and_split_audio(video_path, output_folder, segment_length=10):\n",
        "    audio_path = os.path.join(output_folder, 'audio.wav')\n",
        "    os.system(f\"ffmpeg -i {video_path} -vn -acodec pcm_s16le -ar 44100 -ac 1 {audio_path}\")\n",
        "    y, sr = librosa.load(audio_path, sr=44100)\n",
        "    duration = librosa.get_duration(y=y, sr=sr)\n",
        "    for i in range(0, int(duration), segment_length):\n",
        "        start = i * sr\n",
        "        end = min((i + segment_length) * sr, len(y))\n",
        "        segment = y[start:end]\n",
        "        segment_path = os.path.join(output_folder, f\"segment_{i}.wav\")\n",
        "        librosa.output.write_wav(segment_path, segment, sr)\n",
        "\n",
        "for emotion_folder in os.listdir(video_dir):\n",
        "    emotion_path = os.path.join(video_dir, emotion_folder)\n",
        "    if os.path.isdir(emotion_path):\n",
        "        output_emotion_folder = os.path.join(audio_dataset_dir, emotion_folder)\n",
        "        os.makedirs(output_emotion_folder, exist_ok=True)\n",
        "        for video_file in os.listdir(emotion_path):\n",
        "            if video_file.endswith('.mp4'):\n",
        "                video_path = os.path.join(emotion_path, video_file)\n",
        "                extract_and_split_audio(video_path, output_emotion_folder)\n",
        "\n",
        "print(\"Audio dataset generated successfully.\")"
      ],
      "metadata": {
        "id": "xt_RfmXR_-yx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 5: Convert Audio to SPECTROGRAM Images**"
      ],
      "metadata": {
        "id": "a0KfKQtL_XTM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "Converts each audio segment into a mel-spectrogram image using Librosa.\n",
        "Saves spectrograms into emotion-specific subfolders within SPECTROGRAM_DATASET."
      ],
      "metadata": {
        "id": "50BDcWLoAG7R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell : Generate Spectrogram Images\n",
        "spectrogram_dataset_dir = '/content/drive/MyDrive/Arwan IRIS/SPECTROGRAM_DATASET'\n",
        "os.makedirs(spectrogram_dataset_dir, exist_ok=True)\n",
        "\n",
        "def generate_spectrogram(audio_path, output_image_path):\n",
        "    y, sr = librosa.load(audio_path)\n",
        "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.specshow(S_dB, sr=sr, x_axis='time', y_axis='mel')\n",
        "    plt.axis('off')\n",
        "    plt.savefig(output_image_path, bbox_inches='tight', pad_inches=0)\n",
        "    plt.close()\n",
        "\n",
        "for emotion_folder in os.listdir(audio_dataset_dir):\n",
        "    emotion_path = os.path.join(audio_dataset_dir, emotion_folder)\n",
        "    if os.path.isdir(emotion_path):\n",
        "        output_emotion_folder = os.path.join(spectrogram_dataset_dir, emotion_folder)\n",
        "        os.makedirs(output_emotion_folder, exist_ok=True)\n",
        "        for audio_file in os.listdir(emotion_path):\n",
        "            if audio_file.endswith('.wav'):\n",
        "                audio_path = os.path.join(emotion_path, audio_file)\n",
        "                output_image_path = os.path.join(output_emotion_folder, f\"{os.path.splitext(audio_file)[0]}.png\")\n",
        "                generate_spectrogram(audio_path, output_image_path)\n",
        "\n",
        "print(\"Spectrogram dataset generated successfully.\")"
      ],
      "metadata": {
        "id": "sal3C_gpAFNG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 6: Train ResNet152 Model on AUDIO SPECTROGRAM Dataset for Emotion Classification**"
      ],
      "metadata": {
        "id": "zXFAP1vn_Xvj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "Preprocesses spectrogram images with augmentation.\n",
        "Trains a separate ResNet152 model on the spectrogram dataset, including fine-tuning for optimal performance."
      ],
      "metadata": {
        "id": "8cNnfK4sAO4n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell : Load and Preprocess Spectrogram Dataset\n",
        "train_datagen_spectrogram = ImageDataGenerator(\n",
        "    rescale=1./255,\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    validation_split=0.2,\n",
        "    preprocessing_function=tf.keras.applications.resnet.preprocess_input\n",
        ")\n",
        "\n",
        "train_generator_spectrogram = train_datagen_spectrogram.flow_from_directory(\n",
        "    spectrogram_dataset_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='training',\n",
        "    shuffle=True\n",
        ")\n",
        "\n",
        "val_generator_spectrogram = train_datagen_spectrogram.flow_from_directory(\n",
        "    spectrogram_dataset_dir,\n",
        "    target_size=(img_height, img_width),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical',\n",
        "    subset='validation',\n",
        "    shuffle=False\n",
        ")"
      ],
      "metadata": {
        "id": "NMOvqpjwANBs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell : Define and Train ResNet152 Model for Spectrograms\n",
        "model_spectrogram = create_resnet152_model(num_classes)\n",
        "model_spectrogram.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history_spectrogram = model_spectrogram.fit(\n",
        "    train_generator_spectrogram,\n",
        "    steps_per_epoch=train_generator_spectrogram.samples // batch_size,\n",
        "    validation_data=val_generator_spectrogram,\n",
        "    validation_steps=val_generator_spectrogram.samples // batch_size,\n",
        "    epochs=epochs,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "# Fine-tune last 30 layers\n",
        "base_model_spectrogram = model_spectrogram.layers[0]\n",
        "base_model_spectrogram.trainable = True\n",
        "for layer in base_model_spectrogram.layers[:-30]:\n",
        "    layer.trainable = False\n",
        "\n",
        "model_spectrogram.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history_fine_spectrogram = model_spectrogram.fit(\n",
        "    train_generator_spectrogram,\n",
        "    steps_per_epoch=train_generator_spectrogram.samples // batch_size,\n",
        "    validation_data=val_generator_spectrogram,\n",
        "    validation_steps=val_generator_spectrogram.samples // batch_size,\n",
        "    epochs=fine_tune_epochs,\n",
        "    callbacks=callbacks\n",
        ")\n",
        "\n",
        "model_spectrogram.save('/content/drive/MyDrive/Arwan IRIS/resnet152_spectrogram_final.h5')"
      ],
      "metadata": {
        "id": "uxDBz5SsAQ0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 7: Develop Gradio User Interface for Input Capture**"
      ],
      "metadata": {
        "id": "33_lc1xD_YT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "Creates a Gradio interface to capture image and audio inputs.\n",
        "Converts audio to a spectrogram and predicts emotions using both models.\n",
        "Displays predictions with confidence scores."
      ],
      "metadata": {
        "id": "9_0qNLLsAgYa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell : Gradio Interface Setup\n",
        "def predict_emotions(image, audio):\n",
        "    # Preprocess image\n",
        "    img_array = tf.keras.applications.resnet.preprocess_input(img_to_array(image.resize((224, 224))))\n",
        "    img_array = np.expand_dims(img_array, axis=0)\n",
        "    pred_image = model_image.predict(img_array)\n",
        "    pred_class_image = class_names[np.argmax(pred_image)]\n",
        "    confidence_image = np.max(pred_image) * 100\n",
        "\n",
        "    # Process audio to spectrogram\n",
        "    y, sr = librosa.load(audio, sr=44100)\n",
        "    S = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=128)\n",
        "    S_dB = librosa.power_to_db(S, ref=np.max)\n",
        "    plt.figure(figsize=(10, 4))\n",
        "    librosa.display.specshow(S_dB, sr=sr)\n",
        "    plt.axis('off')\n",
        "    spectrogram_path = '/content/temp_spectrogram.png'\n",
        "    plt.savefig(spectrogram_path, bbox_inches='tight', pad_inches=0)\n",
        "    plt.close()\n",
        "\n",
        "    # Preprocess spectrogram\n",
        "    spectrogram_img = load_img(spectrogram_path, target_size=(224, 224))\n",
        "    spectrogram_array = tf.keras.applications.resnet.preprocess_input(img_to_array(spectrogram_img))\n",
        "    spectrogram_array = np.expand_dims(spectrogram_array, axis=0)\n",
        "    pred_spectrogram = model_spectrogram.predict(spectrogram_array)\n",
        "    pred_class_spectrogram = class_names[np.argmax(pred_spectrogram)]\n",
        "    confidence_spectrogram = np.max(pred_spectrogram) * 100\n",
        "\n",
        "    return (f\"Image: {pred_class_image} ({confidence_image:.2f}%)\",\n",
        "            f\"Spectrogram: {pred_class_spectrogram} ({confidence_spectrogram:.2f}%)\")\n",
        "\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Dog Emotion Classification\")\n",
        "    with gr.Row():\n",
        "        image_input = gr.Image(label=\"Capture Image\", type=\"pil\")\n",
        "        audio_input = gr.Audio(label=\"Record Audio\", type=\"filepath\")\n",
        "    with gr.Row():\n",
        "        predict_button = gr.Button(\"Predict\")\n",
        "    with gr.Row():\n",
        "        image_output = gr.Textbox(label=\"Image Prediction\")\n",
        "        spectrogram_output = gr.Textbox(label=\"Spectrogram Prediction\")\n",
        "\n",
        "    predict_button.click(predict_emotions, inputs=[image_input, audio_input],\n",
        "                         outputs=[image_output, spectrogram_output])\n",
        "\n",
        "demo.launch()"
      ],
      "metadata": {
        "id": "WcYfgr6pAW6w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 8: Integrate the Gradio Interface with Raspberry Pi 5**"
      ],
      "metadata": {
        "id": "4ipygy7X_Ysn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Steps:**\n",
        "\n",
        "Deploy the Gradio app on Raspberry Pi 5 by installing dependencies (TensorFlow, Librosa, Gradio).\n",
        "Use the Pi’s camera and microphone for real-time input.\n",
        "Run the script locally on the Pi with demo.launch(server_name=\"0.0.0.0\") for network access."
      ],
      "metadata": {
        "id": "mNC1om2vAmcc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 9: Predict Emotions Using CNN Models via Gradio Interface**"
      ],
      "metadata": {
        "id": "AZKN0oh5_ZGY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Steps:**\n",
        "\n",
        "The Gradio interface in Step 7 already handles predictions.\n",
        "Ensure models are loaded (model_image and model_spectrogram) before launching the interface."
      ],
      "metadata": {
        "id": "O1JJX9vyAtOX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 10: Evaluate Results with Confusion Matrix and Metrics + Convert to TensorFlow Lite**"
      ],
      "metadata": {
        "id": "K1ggor9__Znj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "Generates confusion matrices to evaluate model performance.\n",
        "Converts both models to TensorFlow Lite with float16 quantization for edge deployment."
      ],
      "metadata": {
        "id": "NqFfPWmiBCJJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cell : Evaluate and Visualize Results\n",
        "# Image model evaluation\n",
        "val_generator.reset()\n",
        "preds_image = np.argmax(model_image.predict(val_generator), axis=1)\n",
        "true_labels_image = val_generator.classes\n",
        "cm_image = confusion_matrix(true_labels_image, preds_image)\n",
        "disp_image = ConfusionMatrixDisplay(confusion_matrix=cm_image, display_labels=class_names)\n",
        "disp_image.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix - Image Model')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "N1VRkGCqA_3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Spectrogram model evaluation\n",
        "val_generator_spectrogram.reset()\n",
        "preds_spectrogram = np.argmax(model_spectrogram.predict(val_generator_spectrogram), axis=1)\n",
        "true_labels_spectrogram = val_generator_spectrogram.classes\n",
        "cm_spectrogram = confusion_matrix(true_labels_spectrogram, preds_spectrogram)\n",
        "disp_spectrogram = ConfusionMatrixDisplay(confusion_matrix=cm_spectrogram, display_labels=class_names)\n",
        "disp_spectrogram.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix - Spectrogram Model')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "SaQVUFk5G1Jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to TensorFlow Lite\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_image)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "tflite_model_image = converter.convert()\n",
        "with open('/content/drive/MyDrive/Arwan IRIS/resnet152_image.tflite', 'wb') as f:\n",
        "    f.write(tflite_model_image)\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_keras_model(model_spectrogram)\n",
        "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
        "converter.target_spec.supported_types = [tf.float16]\n",
        "tflite_model_spectrogram = converter.convert()\n",
        "with open('/content/drive/MyDrive/Arwan IRIS/resnet152_spectrogram.tflite', 'wb') as f:\n",
        "    f.write(tflite_model_spectrogram)\n",
        "\n",
        "print(\"TFLite models saved successfully.\")"
      ],
      "metadata": {
        "id": "sM3fBnwyG2bq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "YYs8QZfRBMbC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 11: Evaluate and Visualize Model Results**"
      ],
      "metadata": {
        "id": "_e3NA_mj_aII"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Step 11: Evaluate and Visualize Model Results\n",
        "# =============================\n",
        "# Define a helper function to evaluate a model using a confusion matrix.\n",
        "def evaluate_model(model, generator, class_labels, title=\"Confusion Matrix\"):\n",
        "    \"\"\"\n",
        "    Evaluates the model on a given data generator and displays the confusion matrix.\n",
        "\n",
        "    Parameters:\n",
        "        model: Trained Keras model.\n",
        "        generator: Data generator (validation/test).\n",
        "        class_labels: List of class names.\n",
        "        title (str): Title for the confusion matrix plot.\n",
        "    \"\"\"\n",
        "    generator.reset()\n",
        "    preds = np.argmax(model.predict(generator), axis=1)\n",
        "    true_labels = generator.classes\n",
        "    cm = confusion_matrix(true_labels, preds)\n",
        "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_labels)\n",
        "    disp.plot(cmap=plt.cm.Blues)\n",
        "    plt.title(title)\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.show()\n",
        "\n",
        "# Evaluate the image model\n",
        "print(\"Evaluating Image Model:\")\n",
        "evaluate_model(image_model, val_generator, class_names, title=\"Image Model Confusion Matrix\")\n",
        "# Evaluate the audio model\n",
        "print(\"Evaluating Audio Model:\")\n",
        "evaluate_model(audio_model, val_audio_gen, audio_class_names, title=\"Audio Model Confusion Matrix\")\n"
      ],
      "metadata": {
        "id": "OsAGxvQNBoXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 12: Develop Gradio Interface for Real-Time Input Capture and Prediction**"
      ],
      "metadata": {
        "id": "QvfjTEqU_aom"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Step 12: Develop Gradio Interface for Real-Time Input Capture and Prediction\n",
        "# =============================\n",
        "# This function defines how the system processes an uploaded image and audio file.\n",
        "# For the audio input, the file is saved, converted to a spectrogram, and then passed to the audio model.\n",
        "def predict_emotions(input_image, input_audio):\n",
        "    # Process image input:\n",
        "    # Resize and preprocess the uploaded image\n",
        "    img = cv2.resize(np.array(input_image), (img_height, img_width))\n",
        "    img = tf.keras.applications.resnet.preprocess_input(img)\n",
        "    img = np.expand_dims(img, axis=0)\n",
        "    pred_img = image_model.predict(img)\n",
        "    emotion_img = class_names[np.argmax(pred_img)]\n",
        "    confidence_img = np.max(pred_img) * 100\n",
        "\n",
        "    # Process audio input:\n",
        "    # Save the uploaded audio to a temporary file\n",
        "    audio_path = \"temp_audio.wav\"\n",
        "    with open(audio_path, \"wb\") as f:\n",
        "        f.write(input_audio.read())\n",
        "    # Convert the audio file to a spectrogram image\n",
        "    spec_path = \"temp_spec.png\"\n",
        "    audio_to_spectrogram(audio_path, spec_path)\n",
        "    # Load and preprocess the spectrogram image\n",
        "    spec_img = load_img(spec_path, target_size=(img_height, img_width))\n",
        "    spec_array = img_to_array(spec_img)\n",
        "    spec_array = tf.keras.applications.resnet.preprocess_input(spec_array)\n",
        "    spec_array = np.expand_dims(spec_array, axis=0)\n",
        "    pred_audio = audio_model.predict(spec_array)\n",
        "    emotion_audio = audio_class_names[np.argmax(pred_audio)]\n",
        "    confidence_audio = np.max(pred_audio) * 100\n",
        "\n",
        "    # Return a fusion of both predictions as a result string.\n",
        "    result = (\n",
        "        f\"Image Emotion: {emotion_img} ({confidence_img:.2f}%)\\n\"\n",
        "        f\"Audio Emotion: {emotion_audio} ({confidence_audio:.2f}%)\"\n",
        "    )\n",
        "    return result\n",
        "\n",
        "# Create the Gradio interface with two inputs: an image and an audio file.\n",
        "iface = gr.Interface(\n",
        "    fn=predict_emotions,\n",
        "    inputs=[\n",
        "        gr.inputs.Image(label=\"Capture/Upload Image\"),\n",
        "        gr.inputs.Audio(source=\"upload\", type=\"file\", label=\"Record/Upload Audio\")\n",
        "    ],\n",
        "    outputs=\"text\",\n",
        "    title=\"Emotion Classification from Image and Audio\",\n",
        "    description=\"Uses ResNet152-based CNN models to predict emotions from facial expressions and voice (via audio spectrogram).\"\n",
        ")\n",
        "# Launch the Gradio interface for real-time testing.\n",
        "iface.launch()"
      ],
      "metadata": {
        "id": "8KShrbdTBzES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Step 13: Raspberry Pi Integration Snippet [OPTIONAL]**"
      ],
      "metadata": {
        "id": "2Y_72Hj9_bFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# =============================\n",
        "# Step 13: (Optional) Raspberry Pi Integration Snippet\n",
        "# =============================\n",
        "# The code below shows how you might capture an image using a PiCamera on a Raspberry Pi.\n",
        "# In an actual deployment on Raspberry Pi hardware, uncomment and use this snippet.\n",
        "\n",
        "from picamera import PiCamera\n",
        "def capture_image_with_picamera():\n",
        "    camera = PiCamera()\n",
        "    camera.resolution = (1024, 768)\n",
        "    camera.start_preview()\n",
        "    time.sleep(2)  # Allow the camera to adjust to lighting conditions\n",
        "    image_path = \"captured_image.jpg\"\n",
        "    camera.capture(image_path)\n",
        "    camera.stop_preview()\n",
        "    camera.close()\n",
        "    return image_path"
      ],
      "metadata": {
        "id": "rO8CE7mKB6Rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "---\n",
        "---"
      ],
      "metadata": {
        "id": "0KCjQJzPB_sj"
      }
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "machine_shape": "hm",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}